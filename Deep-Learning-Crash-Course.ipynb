{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Learning Crash Course.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM723EGjC+QRqP2Yqsuf/ue",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PerceptronV/DLCC/blob/master/Deep-Learning-Crash-Course.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyFs-njZfYpZ",
        "colab_type": "text"
      },
      "source": [
        "#DLCC-10\n",
        "####A 10 minute crash course in Deep Learning\n",
        "\n",
        "/_By Vincent Song_\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R76bt0FavSGW",
        "colab_type": "text"
      },
      "source": [
        "<table>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/PerceptronV/DCLL/blob/master/Deep-Learning-Crash-Course.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/PerceptronV/DCLL/blob/master/Deep-Learning-Crash-Course.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YzwKE_eU9WZZ",
        "colab_type": "text"
      },
      "source": [
        "##Introduction\n",
        "\n",
        "\n",
        "> “Thou shalt not make a machine in the likeness of a human mind.”\n",
        "> \n",
        "> _Frank Herbert - Dune_\n",
        "\n",
        "Artificial intelligence has taken the world by storm. But how exacty does it work, and what lies ahead?\n",
        "\n",
        "We'll be exploring all these and some more. By the end of this quick, interactive notebook, you'll have have trained your own text-generating AI!\n",
        "\n",
        "Let's take a glimpse of greatness:\n",
        "![A blend of human and machine-generated Haikus](https://github.com/PerceptronV/Miscellaneous/raw/master/haiku_guess.PNG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qza4MfAvCF1",
        "colab_type": "text"
      },
      "source": [
        "###A Map of AI\n",
        "![3 Stages of AI](https://image.slidesharecdn.com/futureofai20190507v7-190507093643/95/future-of-ai-20190507-v7-5-638.jpg?cb=1557221878)\n",
        "\n",
        "_After Dario Gil, IBM_\n",
        "\n",
        "| Narrow AI | Broad AI | General AI |\n",
        "| ------------- | ------------- | ------------- |\n",
        "| Single-task, single-domain | Multi-task, multi-domain | Cross-domain learning and reasoning |\n",
        "| Superhuman performace on specific tasks | Explainable | Broad autonomy, superhuman performace in every area |\n",
        "\n",
        "\\\n",
        "\n",
        "\\\n",
        "\n",
        "![The map of AI](https://top6sites.com/wp-content/uploads/2020/05/AI-vs-ML-vs-Deep-Learning.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGixjPGBCrqa",
        "colab_type": "text"
      },
      "source": [
        "##A bit on theory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjMoVOZZXVDf",
        "colab_type": "text"
      },
      "source": [
        "###Let's get our hands dirty!\n",
        "_Time for the code to come in!_\n",
        "\n",
        "For this simple demonstration, we will be coding in Python - the leading language for data scientists."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFy7x46c-vre",
        "colab_type": "text"
      },
      "source": [
        "###Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flIeL0wXfNIk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let us first import the libraries we need\n",
        "import time, os, math\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdOzm6dMf_nc",
        "colab_type": "text"
      },
      "source": [
        "###Creating a simple neuron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DNpJiuNgCEG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def neuron(data, weights, bias):\n",
        "  return data * weight + bias"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qivEIwu0kEu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = np.array([1,2])\n",
        "weights = np.array([0.1, 0.3])\n",
        "bias = 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iN0A2P8R0vCx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "neuron(data, weights, bias)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rvICge0n9bSY",
        "colab_type": "text"
      },
      "source": [
        "##Creating our AI system\n",
        "\n",
        "__SPOILER ALERT__: Implementing a multi-layer neural network with RNN and then training it from scratch is going to take an absurdly long time. It is well beyond what we can do in 10 minutes. Therefore, to speed up the process, we will be using _TensorFlow_, a popular machine learning library, and _Keras_, a framework built on top of it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uk7d6nxjiN0A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import TensorFlow and Keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N05NDOe__FKx",
        "colab_type": "text"
      },
      "source": [
        "###Downloading the data\n",
        "We are going to teach an AI how to generate realistic-looking text of a certain type based on data in a _.txt_ file. Here are few prepared styles for you to choose from: \n",
        "\n",
        "\n",
        "| Text Type  | Description | Adress / URL |\n",
        "| ------------- | ------------- | ------------- |\n",
        "| Haikus | A form of 3-line poetry, originating from Japan. | [https://raw.githubusercontent.com/PerceptronV/Apollo-Psi/master/Haikus.txt](https://raw.githubusercontent.com/PerceptronV/Apollo-Psi/master/Haikus.txt) |\n",
        "| Shakespeare | _Everyone knows him_ | [https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt](https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt) |\n",
        "| Emily Dickinson | American poet. Her works were unconventional and often centred around themes like death or religion. | [https://raw.githubusercontent.com/PerceptronV/Miscellaneous/master/Emily_Dickinson.txt](https://raw.githubusercontent.com/PerceptronV/Miscellaneous/master/Emily_Dickinson.txt) |\n",
        "| Virgil | __Latin__ full text from the Roman epic poem _The Aeneid_ by Virgil | [https://raw.githubusercontent.com/PerceptronV/Miscellaneous/master/aeneid-lat.txt](https://raw.githubusercontent.com/PerceptronV/Miscellaneous/master/aeneid-lat.txt) |\n",
        "\n",
        "\n",
        "\n",
        "Choose the type you'd like, copy its link address, then run the next block and paste in the url.\n",
        "\n",
        "_(To customise, you can also input an url of any text file you like, as long as it is encoded in utf-8 and is public.)_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4QvG7iiAODz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "URL = input(\"Enter the URL of the text file you'd like to use:\\n\")\n",
        "\n",
        "if os.path.exists('src.txt'):\n",
        "  os.remove('src.txt')\n",
        "fp = keras.utils.get_file('src.txt', URL, cache_subdir = '/content')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2_6p2ZvbSu8",
        "colab_type": "text"
      },
      "source": [
        "###Read and tokenize the text\n",
        "Before we do anything, we have to load the text in the file you've downloaded. Then we convert all the characters in that text into integers, called _tokens_. The whole process is called _tokenization_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5cyNk0myXjij",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = open(fp,'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "vocab = sorted(set(text))\n",
        "TOKEN_SIZE = len(vocab)\n",
        "\n",
        "get_tok = {u:i for i, u in enumerate(vocab)}\n",
        "get_char = np.array(vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nz19neRUaD-8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's visualize the mapping between the first set of tokens and characters\n",
        "# \\n is the newline character\n",
        "\n",
        "print('token\\t----->\\tcorresponding character\\n')\n",
        "for i in range(0,7):\n",
        "  print('{}\\t----->\\t{}'.format(i, get_char[i].replace('\\n', '\\\\n')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6NTltYhdxtm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining helper functions to make tokenizing and decoding easier\n",
        "\n",
        "def tokenize(s):\n",
        "  return [get_tok[i] for i in s]\n",
        "\n",
        "def decode(l):\n",
        "  ret=''\n",
        "  for i in l:\n",
        "    ret+=get_char[i]\n",
        "  return ret"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYpzfxojcUJt",
        "colab_type": "text"
      },
      "source": [
        "###Creating a dataset\n",
        "The training process is really simple. We separate the text into sequences of tokens, all of the same length. We then give the model an input token, and train it to predict the most probable next token.\n",
        "\n",
        "This is like the next-word prediction function on your smartphones, just specialized in the text you're giving it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80lCWotIbP4w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converting the whole text into tokens and turning it into tf.data.Dataset\n",
        "\n",
        "token_text = tokenize(text)\n",
        "token_data = tf.data.Dataset.from_tensor_slices(token_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ft_WHyKdKbw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Breaking the text into sequences\n",
        "\n",
        "SEQ_LEN = 100   # This is the sequence length. You can tweak this value.\n",
        "\n",
        "seq_data = token_data.batch(SEQ_LEN, drop_remainder = True)\n",
        "\n",
        "print('Example sequence:\\n')\n",
        "for i in seq_data.take(1):\n",
        "  print(decode(i).replace('\\n', '\\\\n'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5d8ktDn9eOhl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SZ = 64  # The batch size parameter. It's usually between 64-256.\n",
        "\n",
        "# Creating input-target pairs\n",
        "\n",
        "def to_inp_targ(seq):\n",
        "  '''\n",
        "    Function for turning an individual sequence into input-target pairs\n",
        "  '''\n",
        "  return seq[:-1], seq[1:]\n",
        "\n",
        "# Applying the to_inp_targ function to all sequences, then batching the dataset\n",
        "dataset = seq_data.map(to_inp_targ)\n",
        "batch_dataset = dataset.batch(BATCH_SZ, drop_remainder = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNjBAlzxxDxR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Visualizing input-target pairs\n",
        "\n",
        "for inp_seq, out_seq in dataset.take(1):\n",
        "  print('Input')\n",
        "  print(decode(inp_seq).replace('\\n', '\\\\n'))\n",
        "  print('\\nOutput')\n",
        "  print(decode(out_seq).replace('\\n', '\\\\n'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wsdoi4Fdft-F",
        "colab_type": "text"
      },
      "source": [
        "###Making the model\n",
        "We're going to make our own on top of the keras.Model framework.\n",
        "\n",
        "The model takens in a token, turns it into an embedding vector, then passes it through a Recurrent Neural Network, a GRU in this case. This then goes through a fully connected layer, the size of which equals the number of tokens (i.e. the number of different characters we have in our text).\n",
        "\n",
        "The size of the fully connected layer is the same as the token size, because we want the model to output a probability for all the possible next tokens, hence achieving next-word prediction.\n",
        "\n",
        "Once the model predicts the next token, we will feed in the correct next token from the data, instead of using the token which the model predicts. This is called teacher-forcing, which makes training faster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvhZ6GL8jETZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model parameters\n",
        "\n",
        "EMB_DIM = 256  # This is the size of the embedding vector\n",
        "UNITS = 1024  # This is the number of RNN units in the model\n",
        "\n",
        "'''\n",
        "0 <= dropout < 1\n",
        "\n",
        "Dropout controls how much the model forgets certain information \n",
        "during training.\n",
        "\n",
        "I.e.  A dropout of 0 means that the model remembers everything its \n",
        "      taught at evert moment in the training.\n",
        "\n",
        "Dropout is used to combat overfitting, which happens when your model\n",
        "learns the data so well it begins to memorize it. \n",
        "\n",
        "So, if you find that after training below, the model memorises the\n",
        "dataset its given instead of generating anything new, try increasing\n",
        "the dropout value.\n",
        "'''\n",
        "DROPOUT = 0.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_T_H8ZKtesai",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model(keras.Model):\n",
        "  def __init__(self, batch_size, embedding_dim, units, token_size, dropout):\n",
        "    super(Model,self).__init__(name = 'Model')\n",
        "    \n",
        "    # We now define the layers we need.\n",
        "    # Note: The input layer is only used for declaring the input size;\n",
        "    # as you can see, it isn't really referred to below in the call function.\n",
        "    self.input_layer = keras.Input(batch_shape = (batch_size, None))\n",
        "\n",
        "    self.embedding_layer = keras.layers.Embedding(token_size, embedding_dim)\n",
        "\n",
        "    self.rnn_layer = keras.layers.GRU(units, return_sequences = True,\n",
        "                                      recurrent_initializer = 'glorot_uniform',\n",
        "                                      dropout = dropout, stateful = True)\n",
        "    \n",
        "    self.out = keras.layers.Dense(token_size)\n",
        "\n",
        "\n",
        "  def call(self, inp):\n",
        "    # Defining how the input propagates through our neural network\n",
        "\n",
        "    x = self.embedding_layer(inp)\n",
        "    x = self.rnn_layer(x)\n",
        "    return self.out(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SYUFKKOklo5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize the model\n",
        "model = Model(BATCH_SZ, EMB_DIM, UNITS, TOKEN_SIZE, DROPOUT)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpoCPs9pfSu_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Example output shape:')\n",
        "for i in batch_dataset.take(1):\n",
        "  out = model(i[0])\n",
        "  print(out.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Niy58vGhiQ3F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_UzN5Fpk6ON",
        "colab_type": "text"
      },
      "source": [
        "###Defining the loss function and optimizer\n",
        "Since the task is inherently about classifying the most probable next token, we use the cross-entropy loss.\n",
        "\n",
        "Cross-entropy loss formula:\n",
        "\n",
        "$-\\sum\\limits_{i=1}^M y_i\\log(p_i)$\n",
        "\n",
        "$y_i$ is the label of the $i^{th}$ token. In classification, this value is either 0 or 1, where 1 means that the token is the correct prediction, and 0 states otherwise.\n",
        "\n",
        "$p_i$ is the probability assigned by the model on the $i^{th}$ token.\n",
        "\n",
        "$M$ is the token size.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "As for the optimizer, we will use the Adam Optimizing Algorithm to help train our model weights. More information of Adam is [available here](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/#:~:text=Adam%20is%20an%20optimization%20algorithm,iterative%20based%20in%20training%20data.&text=The%20algorithm%20is%20called%20Adam.)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7A1l9Zvm6WJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Remark: There's nothing magical about the name \n",
        "`sparse_categorical_crossentropy`. It's just some technical\n",
        "details. The loss function is still calculating the \n",
        "cross-entropy loss by the formula given above.\n",
        "'''\n",
        "def loss_function(true, pred):\n",
        "  return keras.losses.sparse_categorical_crossentropy(true, pred,\n",
        "                                                      from_logits = True)\n",
        "\n",
        "'''\n",
        "Learning rate is the size of each gradient descent step. You can fine-\n",
        "tune this parameter to make your training more successful.\n",
        "If the loss decreases too slowly, increase the learning rate. If the\n",
        "loss drops quickly but rises rapidly after a while, decrease the learning\n",
        "rate.\n",
        "'''\n",
        "LEARNING_RATE = 1e-3\n",
        "optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aGlEqkprdeJ",
        "colab_type": "text"
      },
      "source": [
        "###Training our network\n",
        "We are going to implement the training loop and gradient descent here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fdZQMKjsgrP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_step(input, labels):\n",
        "  \n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model(input)\n",
        "    loss = loss_function(labels, predictions)\n",
        "  \n",
        "  # calculate gradients with respect to model variables\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "\n",
        "  # update gradients with optimzer, based on the gradients\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "  return tf.reduce_mean(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCN600Uz8Han",
        "colab_type": "text"
      },
      "source": [
        "We're just iterating over the data in the code below. Nothing mysterious."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dH29hizZtsg5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(epochs):\n",
        "\n",
        "  for i in range(epochs):\n",
        "    total_loss = []\n",
        "    start = time.time()\n",
        "\n",
        "    for e, batch in enumerate(batch_dataset):\n",
        "      # Due to the way the dataset is structured, each batch includes\n",
        "      # two components: batch[0] is the inputs, batch[1] is the labels\n",
        "\n",
        "      step_loss = train_step(batch[0], batch[1])\n",
        "      total_loss.append(step_loss)\n",
        "\n",
        "      if (e + 1) % 10 == 0:\n",
        "        print('Batch: {}\\tElapsed time: {}s\\tLoss: {}'.format(\n",
        "            e + 1, time.time()-start, np.mean(np.asarray(total_loss))\n",
        "        ))\n",
        "    \n",
        "    print('> Epoch: {}\\tTime: {}s\\tLoss: {}\\n'.format(\n",
        "        i+1, time.time()-start, np.mean(np.asarray(total_loss))\n",
        "    ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BykoGmFO97yc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "Epochs is the number of rounds we train an AI. Generally, the more you\n",
        "train, the better. But there is a point at which the model starts to\n",
        "overfit, or in other words, starts to model the data so well it memorises\n",
        "it. \n",
        "\n",
        "If overfitting happens to you, try reducing the number of epochs or tweak\n",
        "the dropout parameter above.\n",
        "\n",
        "If the AI isn't generating meaningul stuff after you've trained it, try\n",
        "increasing the number of epochs, or change the model structure altogether.\n",
        "'''\n",
        "\n",
        "EPOCHS = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y327tlJ_9x7f",
        "colab_type": "text"
      },
      "source": [
        "###_Actually_ running the __`train`__ function\n",
        "Behold - the FUN!!!\n",
        "\n",
        "But, training does take a few minutes. Be patient, good things come with time. Watch the loss drop - it's most satisfying!\n",
        "\n",
        "> “The vision of time is broad, but when you pass through it, time becomes a narrow door.”\n",
        ">\n",
        "> _Frank Herbert - Dune_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bB3Qn1si-iv9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train(EPOCHS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBedZ2u7o1HI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's save our model weights to immortalize it's legacy\n",
        "\n",
        "model.save_weights('model_weights.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYgooUvvC0LJ",
        "colab_type": "text"
      },
      "source": [
        "##Generating text!!\n",
        "Let's get started with our system!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dLZn23jo6Wy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "We have to first instantiate a new model, because we built our\n",
        "first model based on a fized batch size. However, now we need to\n",
        "generated text one sample at a time, so we have to build another\n",
        "model based on a batch size of 1.\n",
        "'''\n",
        "# It's just a technical detail\n",
        "\n",
        "gen_model = Model(1, EMB_DIM, UNITS, TOKEN_SIZE, DROPOUT)\n",
        "gen_model.build((1, None))\n",
        "gen_model.load_weights('model_weights.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evGsAJkrypJk",
        "colab_type": "text"
      },
      "source": [
        "### Sampling\n",
        "\n",
        "The following code inputs a prompt to the model, and then samples the next predicted token based on their corresponding probabilities, generated by our model.\n",
        "\n",
        "Note that before sampling, the probabilities are divided by a temperature. A higher temperature means there is a higher chance that some tokens with a lower assigned popability might be sampled, often making the generated text more surprising. As the temperature approaches 0, it is basically the same as selecting the token with highest probability.\n",
        "\n",
        "Temperature is always a positive integer. Tweak around to generate balance the creativity with the uniformity of the text.\n",
        "\n",
        "### The generaion procedure\n",
        "\n",
        "Once a token is sampled, it is added to the original prompt. This continued prompt is fed back into the model, which then generates next-token-probabilities for us to sample from, and so we have another token. We add this to the original prompt, let the model predict the next token based on the new prompt, and so on etc.\n",
        "\n",
        "This generation technique continues until the model has generated the desired number of outputs, the `generation_length`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrUJKpLkC6rv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(temperature, generation_length, prompt):\n",
        "    \n",
        "    # Turning input text into tokens\n",
        "    input = tf.expand_dims(tokenize(prompt), 0)\n",
        "\n",
        "    output = []  # Initializing a list to store generated tokens\n",
        "    gen_model.reset_states()  # Resetting the idden states of our RNN model\n",
        "\n",
        "    for i in range(generation_length):\n",
        "      \n",
        "      preds = gen_model(input)  # Making a next-word prediction with our model\n",
        "      preds = tf.squeeze(preds, 0)\n",
        "      preds = preds / temperature  # Dividing distribution by temperature\n",
        "\n",
        "      # Sampling from the distribution\n",
        "      id = tf.random.categorical(preds, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      input = tf.expand_dims([id],0)\n",
        "      output.append(id)\n",
        "      \n",
        "    return prompt + decode(output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDE3Wl0ED_iq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining generation parameters\n",
        "\n",
        "TEMPERATURE = 0.2\n",
        "GENERATION_LENGTH = 300"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAhSZ2qO1Emh",
        "colab_type": "text"
      },
      "source": [
        "### Generation!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iC3oDtE6EHrr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prompt = 'jar of fireflies'\n",
        "result = generate_text(TEMPERATURE, GENERATION_LENGTH, prompt)\n",
        "\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cd-Y7f-i1kYx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prompt = '\\n\\na glass of sunset\\n'\n",
        "result = generate_text(TEMPERATURE, GENERATION_LENGTH, prompt)\n",
        "\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}